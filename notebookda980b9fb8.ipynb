{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11897623,"sourceType":"datasetVersion","datasetId":7478849},{"sourceId":11981789,"sourceType":"datasetVersion","datasetId":7535547}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# =========================================\n# 0. PACKAGES & CONSTANTS\n# =========================================\nimport warnings, requests, numpy as np, pandas as pd\nfrom pathlib import Path\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sentence_transformers import SentenceTransformer\nfrom catboost import CatBoostClassifier\n\nwarnings.simplefilter(\"ignore\", RuntimeWarning)\n\nFIN_CSV  =  \"/kaggle/input/dataset/investing_news.csv\"\nGEO_CSV  =  \"/kaggle/input/topperday/top_per_day_news.csv\"\n\nPCA_DIM  = 64          # per block → 128 total","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T11:56:17.076428Z","iopub.execute_input":"2025-05-28T11:56:17.076736Z","iopub.status.idle":"2025-05-28T11:56:17.081721Z","shell.execute_reply.started":"2025-05-28T11:56:17.076714Z","shell.execute_reply":"2025-05-28T11:56:17.080893Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import re\ndef load_fin(path: Path) -> pd.DataFrame:\n    \"\"\"Parse Investing.com dump even when title contains commas.\"\"\"\n    rows = []\n    with open(path, encoding=\"utf-8-sig\") as fh:\n        next(fh)  # skip header\n        for line in fh:\n            line = line.strip().strip(\"\\ufeff\")\n            m = re.match(r\"^(.*?),(https?://[^,]+),(.*)$\", line)\n            if not m:\n                continue\n            title, url, raw = m.groups()\n            date_match = re.search(r\"\\d{2}\\.\\d{2}\\.\\d{4}\", raw)\n            if not date_match:\n                continue\n            dt = pd.to_datetime(date_match.group(), format=\"%d.%m.%Y\", errors=\"coerce\")\n            if pd.isna(dt):\n                continue\n            rows.append((dt.normalize(), title.strip(), url))\n    df = pd.DataFrame(rows, columns=[\"date\", \"title\", \"url\"])\n    return df.assign(src=\"fin\", weight=1.0)\n\n\n\n\ndef load_geo(path: Path) -> pd.DataFrame:\n    df = pd.read_csv(path, encoding=\"utf-8-sig\")\n    # Expect columns: date,title,url,sim  (sim∈[0,1])\n    if \"date\" not in df.columns:\n        raise ValueError(\"В geo‑CSV нет столбца 'date'. Проверьте заголовки.\")\n    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.normalize()\n    df = df.dropna(subset=[\"date\", \"title\"]).copy()\n    df[\"weight\"] = df[\"sim\"].clip(0, 1).fillna(0.0)\n    return df.loc[:, [\"date\", \"title\", \"url\", \"weight\"]].assign(src=\"geo\")\n\n\nfin_news = load_fin(FIN_CSV)\ngeo_news = load_geo(GEO_CSV)\nprint(f\"FIN {len(fin_news):,} | GEO {len(geo_news):,}\")\n\nnews = pd.concat([fin_news, geo_news], ignore_index=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T11:56:21.257173Z","iopub.execute_input":"2025-05-28T11:56:21.257725Z","iopub.status.idle":"2025-05-28T11:56:21.521103Z","shell.execute_reply.started":"2025-05-28T11:56:21.257702Z","shell.execute_reply":"2025-05-28T11:56:21.520340Z"}},"outputs":[{"name":"stdout","text":"FIN 1,343 | GEO 2,311\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"\n# =========================================\n# 2. SENTENCE EMBEDDINGS & SIMPLE SENTIMENT\n# =========================================\nst_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n\nPOS = (\"рост\",\"прибавил\",\"увелич\",\"рекорд\")\nNEG = (\"падени\",\"снизил\",\"упал\",\"минимум\")\n\nnews[\"sent\"] = news[\"title\"].str.lower().apply(\n    lambda t: np.tanh((sum(w in t for w in POS) - sum(w in t for w in NEG))/2)\n)\nnews[\"emb\"] = list(st_model.encode(news[\"title\"].tolist(), batch_size=128, show_progress_bar=False))\n\n# =========================================\n# 3. DAILY AGGREGATION (separate FIN / GEO)\n# =========================================\n\ndef aggregate_block(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Return daily DF with weighted mean embedding & sentiment.\"\"\"\n    def agg_day(day_df):\n        w = day_df[\"weight\"].to_numpy()\n        sent_w = np.average(day_df[\"sent\"], weights=w)\n        emb_stack = np.average(np.vstack(day_df[\"emb\"]), axis=0, weights=w)\n        return pd.Series({\"sent_mean\": sent_w, \"n\": len(day_df), \"emb\": emb_stack})\n    out = df.groupby(\"date\").apply(agg_day)\n    return out\n\nfin_day = aggregate_block(news[news.src==\"fin\"].copy())\ngeo_day = aggregate_block(news[news.src==\"geo\"].copy())\n\nprint(f\"fin_day rows {len(fin_day)}, geo_day {len(geo_day)}\")\n\n# =========================================\n# 4. PCA‑64 PER BLOCK\n# =========================================\n\ndef pca_stack(day_df: pd.DataFrame, prefix: str) -> pd.DataFrame:\n    emb_mat = np.vstack(day_df[\"emb\"].values)\n    pca = PCA(n_components=PCA_DIM, random_state=42).fit(emb_mat)\n    comps = pca.transform(emb_mat)\n    cols = [f\"{prefix}p{i}\" for i in range(PCA_DIM)]\n    return pd.concat([day_df.drop(columns=[\"emb\"]), pd.DataFrame(comps, index=day_df.index, columns=cols)], axis=1)\n\nfin_day = pca_stack(fin_day, \"fin_\")\ngeo_day = pca_stack(geo_day, \"geo_\")\n\n# =========================================\n# 5. SHOCK / LAG / ROLL FEATURES PER BLOCK\n# =========================================\nfor df, pref in [(fin_day,\"fin_\"),(geo_day,\"geo_\")]:\n    roll = df[\"sent_mean\"].rolling(30, min_periods=10)\n    z = (df[\"sent_mean\"] - roll.mean())/roll.std(ddof=0)\n    df[f\"{pref}shock\"] = (z.abs()>1.8).astype(int)\n    df[f\"{pref}shock_mag\"] = z.abs().fillna(0)\n    for lag in [1,2,3]:\n        df[f\"{pref}sent_lag{lag}\"] = df[\"sent_mean\"].shift(lag)\n        df[f\"{pref}shock_lag{lag}\"] = df[f\"{pref}shock\"].shift(lag)\n    df[f\"{pref}sent_roll7\"] = df[\"sent_mean\"].rolling(7).mean()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T11:56:24.800844Z","iopub.execute_input":"2025-05-28T11:56:24.801481Z","iopub.status.idle":"2025-05-28T11:56:38.640752Z","shell.execute_reply.started":"2025-05-28T11:56:24.801459Z","shell.execute_reply":"2025-05-28T11:56:38.640008Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"470f5132327a4bf5a9857e552ee89cd8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6b0cee2d872402b8c70fd4756bd339b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/3.89k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e533399679e94df7a328ca3c8b18b90b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de6e1121116445e195e76fc905c80282"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25af9f07cb5a46219e6e2e47c072c135"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fd2b16dcc284e56a1f64f41eaed194f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef0c8d3086ce45239620b34625c35904"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67ea417fefc04c6e8eebd298b2dbd24a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5fcfe72739240319242e85d02d2d3a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72b835bdd843442baa3f136dcb63abbf"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_35/3170603501.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  out = df.groupby(\"date\").apply(agg_day)\n/tmp/ipykernel_35/3170603501.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  out = df.groupby(\"date\").apply(agg_day)\n","output_type":"stream"},{"name":"stdout","text":"fin_day rows 724, geo_day 2311\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# =========================================\n# 6. CANDLES & TECHNICALS\n# =========================================\n\ndef candles(sec:str,start:str,end:str):\n    url = f\"https://iss.moex.com/iss/engines/stock/markets/shares/securities/{sec}/candles.json\"\n    pars = {\"from\":start, \"till\":end, \"interval\":24, \"start\":0}\n    keep=[\"end\",\"open\",\"high\",\"low\",\"close\",\"value\"]\n    frames=[]\n    while True:\n        js=requests.get(url,params=pars,timeout=20).json()[\"candles\"]\n        if not js[\"data\"]: break\n        cols=[c.lower() for c in js[\"columns\"]]\n        frames.append(pd.DataFrame(js[\"data\"],columns=cols)[keep])\n        pars[\"start\"]+=len(frames[-1])\n    df=pd.concat(frames,ignore_index=True)\n    df.columns=[\"date\",\"open\",\"high\",\"low\",\"close\",\"volume\"]\n    df[\"date\"]=pd.to_datetime(df[\"date\"]).dt.normalize()\n    return df.set_index(\"date\").sort_index()\n\nSTART, END = \"2014-01-01\", \"2025-05-21\"\nprices = candles(\"MOEX\",START,END)\nprices[\"ret1\"] = prices.close.pct_change()\nprices[\"sma5\"] = prices.close.rolling(5).mean()\nprices[\"sma20\"] = prices.close.rolling(20).mean()\nprices[\"vol_z\"] = (prices.volume - prices.volume.rolling(20).mean())/prices.volume.rolling(20).std(ddof=0)\nprices[\"atr14\"] = (prices.high - prices.low).rolling(14).mean()/prices.close\nprices = prices.dropna()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T11:57:43.606896Z","iopub.execute_input":"2025-05-28T11:57:43.607483Z","iopub.status.idle":"2025-05-28T11:57:48.472756Z","shell.execute_reply.started":"2025-05-28T11:57:43.607457Z","shell.execute_reply":"2025-05-28T11:57:48.472033Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# =========================================\n# 7. MERGE + SHIFT + TARGET\n# =========================================\nfeat = prices.join(fin_day, how=\"left\").join(geo_day, how=\"left\", rsuffix=\"_r\").fillna(0)\n# shift all news-derived cols by +1 trading day\nnews_cols = [c for c in feat.columns if c.startswith(\"fin_\") or c.startswith(\"geo_\")]\nfeat[news_cols] = feat[news_cols].shift(1)\n\nfeat[\"vol_next\"] = ((prices.high - prices.low)/prices.close).shift(-1)\nthr = feat.vol_next.median()\nfeat[\"vol_cls\"] = (feat.vol_next > thr).astype(int)\n\nfeat[\"weekday\"] = feat.index.weekday\nfeat[\"month\"] = feat.index.month\nfeat = feat.dropna()\nprint(f\"dataset {feat.shape} — high‑σ share {feat.vol_cls.mean():.3f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T11:57:51.731614Z","iopub.execute_input":"2025-05-28T11:57:51.731904Z","iopub.status.idle":"2025-05-28T11:57:51.781064Z","shell.execute_reply.started":"2025-05-28T11:57:51.731882Z","shell.execute_reply":"2025-05-28T11:57:51.780276Z"}},"outputs":[{"name":"stdout","text":"dataset (2852, 164) — high‑σ share 0.500\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_35/1597990577.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  feat[\"vol_next\"] = ((prices.high - prices.low)/prices.close).shift(-1)\n/tmp/ipykernel_35/1597990577.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  feat[\"vol_cls\"] = (feat.vol_next > thr).astype(int)\n/tmp/ipykernel_35/1597990577.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  feat[\"weekday\"] = feat.index.weekday\n/tmp/ipykernel_35/1597990577.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  feat[\"month\"] = feat.index.month\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# =========================================\n# 8. CATBOOST CV\n# =========================================\nX = feat.drop(columns=[\"vol_next\",\"vol_cls\"])\ny = feat.vol_cls\ncat_idx = [i for i,c in enumerate(X.columns) if c in (\"weekday\",\"month\")]\n\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import precision_recall_curve, classification_report\nimport numpy as np\n\ncv = TimeSeriesSplit(5)\nprobas_all, y_all = [], []\n\nfor tr, te in cv.split(X):\n    model = CatBoostClassifier(\n        iterations=400, depth=7, learning_rate=0.05,\n        colsample_bylevel=0.8, random_seed=42, verbose=0,\n        loss_function=\"Logloss\", auto_class_weights=\"Balanced\")\n    model.fit(X.iloc[tr], y.iloc[tr], cat_features=cat_idx)\n\n    probas = model.predict_proba(X.iloc[te])[:, 1]   # P(class=high)\n    probas_all.append(probas)\n    y_all.append(y.iloc[te].values)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T11:57:59.514947Z","iopub.execute_input":"2025-05-28T11:57:59.515739Z","iopub.status.idle":"2025-05-28T11:58:51.289756Z","shell.execute_reply.started":"2025-05-28T11:57:59.515714Z","shell.execute_reply":"2025-05-28T11:58:51.289167Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"\n# ---------- 1. подбираем τ на валидационных фолдах ----------\nprobas_all = np.concatenate(probas_all)\ny_all      = np.concatenate(y_all)\n\nprecision, recall, thr = precision_recall_curve(y_all, probas_all)\n\ntarget_rec = 0.65                          # вашу цель можно менять\nidx = np.where(recall >= target_rec)[0][-1]\ntau = thr[idx]\nprint(f\"Выбранный τ = {tau:.2f}  при recall ≥ {recall[idx]:.3f}\")\n\n# ---------- 2. применяем τ и считаем метрики ----------\ny_pred = (probas_all >= tau).astype(int)\nprint(classification_report(y_all, y_pred, digits=3,\n                            target_names=[\"low\",\"high\"]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T11:59:29.468410Z","iopub.execute_input":"2025-05-28T11:59:29.468746Z","iopub.status.idle":"2025-05-28T11:59:29.485425Z","shell.execute_reply.started":"2025-05-28T11:59:29.468723Z","shell.execute_reply":"2025-05-28T11:59:29.484820Z"}},"outputs":[{"name":"stdout","text":"Выбранный τ = 0.33  при recall ≥ 0.650\n              precision    recall  f1-score   support\n\n         low      0.642     0.513     0.570      1305\n        high      0.523     0.650     0.580      1070\n\n    accuracy                          0.575      2375\n   macro avg      0.582     0.582     0.575      2375\nweighted avg      0.588     0.575     0.575      2375\n\n","output_type":"stream"}],"execution_count":10}]}